{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasibulhshovo/COMP432/blob/main/lab_2_excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Implementation defining a simplelayer and trying it out with different activation functions"
      ],
      "metadata": {
        "id": "RYW4RrBMmCXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset\n",
        "data = \"\"\"Day No.,Humidity  (%),Temperature (degree Celsius),Rain\n",
        "1,80,20,Yes\n",
        "2,20,15,No\n",
        "3,70,25,Yes\n",
        "4,30,24,Yes\n",
        "5,55,28,No\n",
        "6,68,19,Yes\n",
        "7,45,21,No\n",
        "8,73,17,Yes\n",
        "9,44,16,No\n",
        "10,56,22,No\"\"\"\n",
        "\n",
        "# Read the string data into a pandas DataFrame using io.StringIO\n",
        "df = pd.read_csv(io.StringIO(data))\n",
        "# Map the 'Rain' column from 'Yes'/'No' to numerical values 1/0\n",
        "df['Rain'] = df['Rain'].map({'Yes': 1, 'No': 0})\n",
        "# Convert the 'Humidity' and 'Temperature' columns to a PyTorch tensor, representing the features (X)\n",
        "X = torch.tensor(df[['Humidity  (%)', 'Temperature (degree Celsius)']].values, dtype=torch.float32)\n",
        "# Convert the 'Rain' column to a PyTorch tensor, representing the labels (y), and add a dimension with .unsqueeze(1)\n",
        "y = torch.tensor(df['Rain'].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Add Gaussian noise to features (relates to prob PDF section)\n",
        "mean = 0 # Define the mean for the Gaussian noise\n",
        "std = 0.1 # Define the standard deviation for the Gaussian noise\n",
        "# Generate a tensor of random numbers from a normal distribution with the specified mean and standard deviation\n",
        "noise = torch.normal(mean, std, size=X.shape)\n",
        "# Add the generated noise to the original feature tensor\n",
        "X_noisy = X + noise\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# Use train_test_split to get the indices for the training and testing data\n",
        "train_idx, test_idx = train_test_split(range(len(X_noisy)), test_size=0.2, random_state=42)\n",
        "# Use the indices to get the training features\n",
        "X_train = X_noisy[train_idx]\n",
        "# Use the indices to get the training labels\n",
        "y_train = y[train_idx]\n",
        "# Use the indices to get the testing features\n",
        "X_test = X_noisy[test_idx]\n",
        "# Use the indices to get the testing labels\n",
        "y_test = y[test_idx]"
      ],
      "metadata": {
        "id": "x2Bu1uvamQBm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are Activation Functions?**\n",
        "Think of an activation function as a decision-maker inside a neuron. In our brains, a neuron only \"fires\" (sends a signal) if the combined input signals it receives are strong enough.\n",
        "\n",
        "In a neural network, the activation function does the same thing. It takes the output of a neuron and decides whether that signal is important enough to be passed on to the next layer. This is how the network learns to make complex, non-linear decisions. Without activation functions, a neural network is just a bunch of simple multiplications and additions, making it useless for solving anything but the simplest problems.\n",
        "\n",
        "**Our Example:** Deciding if it will Rain\n",
        "Let's imagine a neural network that predicts if it will rain based on humidity and temperature.\n",
        "\n",
        "Humidity: 80%\n",
        "\n",
        "Temperature: 20°C\n",
        "\n",
        "The neuron takes these two numbers, multiplies them by some learned \"weights,\" and adds them up. Let's say the result is 3.5. This number, 3.5, is the input to our activation function.\n",
        "\n",
        "Now, let's see how different activation functions decide what to do with this 3.5.\n",
        "\n",
        "### **1. ReLU (Rectified Linear Unit)**\n",
        "The ReLU function is like a simple switch.\n",
        "\n",
        "**Rule:** If the input is positive, let it pass through as is. If it's zero or negative, stop the signal completely (output 0).\n",
        "\n",
        "**Our Example:** Our input is 3.5, which is positive. So, ReLU outputs 3.5.\n",
        "\n",
        "**Decision:** \"The signal is strong, pass it on!\"\n",
        "\n",
        "### **2. Sigmoid**\n",
        "The Sigmoid function is like a probability dial. It squishes any number into a value between 0 and 1.\n",
        "\n",
        "**Rule:** It takes any number and turns it into a decimal between 0 and 1. A number like -10 becomes close to 0, and a number like +10 becomes close to 1.\n",
        "\n",
        "**Our Example:** Our input is 3.5. The Sigmoid function turns this into a number around 0.97.\n",
        "\n",
        "**Decision:** \"This is a very strong signal. It's almost certain (0.97) that we should predict 'Yes, it will rain!'\"\n",
        "\n",
        "### **3. Tanh (Hyperbolic Tangent)**\n",
        "The Tanh function is similar to Sigmoid but outputs a value between -1 and 1.\n",
        "\n",
        "**Rule:** It takes any number and converts it to a value between -1 and 1. Positive numbers move towards 1, and negative numbers move towards -1.\n",
        "\n",
        "**Our Example:** Our input is 3.5. The Tanh function turns this into a number around 0.99.\n",
        "\n",
        "**Decision:** \"This is a very strong positive signal, indicating a strong 'Yes'.\" The negative outputs could indicate a strong 'No'.\n",
        "\n",
        "### **4. SiLU (Sigmoid Linear Unit)**\n",
        "SiLU is a newer, smoother function.\n",
        "\n",
        "**Rule:** It's a bit more complex. It's the input number multiplied by its own Sigmoid.\n",
        "\n",
        "**Our Example:** Our input is 3.5. SiLU multiplies 3.5 by sigmoid(3.5), which is roughly 0.97. The result is about 3.4.\n",
        "\n",
        "**Decision:** \"The signal is strong, and this function allows for a more subtle, nuanced pass-through of information.\" This smoothness can help deep networks train more effectively.\n",
        "\n",
        "### **5. Leaky ReLU (Leaky Rectified Linear Unit)**\n",
        "The Leaky ReLU function is like a switch that's never fully off. It lets a small amount of signal through, even if it's negative.\n",
        "\n",
        "**Rule:** If the input is positive, let it pass through as is. If the input is zero or negative, let a small fraction of the signal pass through instead of stopping it completely.\n",
        "\n",
        "**Our Example:** If our input is -2, ReLU would output 0. Leaky ReLU would output a small value, like -0.02 (assuming a 'leak' rate of 0.01).\n",
        "\n",
        "**Decision:** \"The signal is negative, but we'll let a little bit of information pass through anyway.\""
      ],
      "metadata": {
        "id": "k0Cw58TQi28H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ExtendedLayer is a custom PyTorch neural network module.\n",
        "# It's a simple two-layer network where the activation function\n",
        "# for the hidden layer can be specified during initialization.\n",
        "class ExtendedLayer(nn.Module):\n",
        "    # __init__ method is the constructor. It initializes the layers\n",
        "    # and sets the activation function.\n",
        "    def __init__(self, input_size=2, hidden_size=4, output_size=1, activation='linear'):\n",
        "        super(ExtendedLayer, self).__init__()\n",
        "        # fc1 is the first fully connected layer (input to hidden).\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # fc2 is the second fully connected layer (hidden to output).\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        # Store the activation function as a lowercase string.\n",
        "        self.activation = activation.lower()\n",
        "\n",
        "    # forward method defines the forward pass of the network.\n",
        "    # It takes an input tensor 'x', applies the first layer,\n",
        "    # then the specified activation function, and finally the second layer.\n",
        "    def forward(self, x):\n",
        "        # Apply the first linear layer.\n",
        "        x = self.fc1(x)\n",
        "        # Apply the chosen activation function.\n",
        "        if self.activation == 'relu':\n",
        "            x = nn.ReLU()(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            x = nn.LeakyReLU()(x)\n",
        "        elif self.activation == 'silu':\n",
        "            x = nn.SiLU()(x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            x = nn.Sigmoid()(x)\n",
        "        elif self.activation == 'tanh':\n",
        "            x = nn.Tanh()(x)\n",
        "        else:\n",
        "            pass  # linear activation (no function applied)\n",
        "        # Apply the second linear layer.\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# This function trains and evaluates the ExtendedLayer model\n",
        "# with a specified activation function.\n",
        "def train_and_evaluate_ext(activation):\n",
        "    # Instantiate the model with the given activation function.\n",
        "    model = ExtendedLayer(activation=activation)\n",
        "    # Define the optimizer (Adam) to update the model's parameters.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    # Choose the loss function.\n",
        "    # BCEWithLogitsLoss is used for binary classification with a Sigmoid output.\n",
        "    # MSELoss (Mean Squared Error) is used otherwise.\n",
        "    criterion = nn.BCEWithLogitsLoss() if activation in ['sigmoid', 'tanh'] else nn.MSELoss()\n",
        "\n",
        "    # Train for 200 epochs. An epoch is one full pass over the training data.\n",
        "    for epoch in range(200):\n",
        "        # Set the model to training mode.\n",
        "        model.train()\n",
        "        # Clear the gradients of all optimized tensors.\n",
        "        optimizer.zero_grad()\n",
        "        # Perform the forward pass to get predictions.\n",
        "        outputs = model(X_train)\n",
        "        # For non-sigmoid/tanh activations, apply sigmoid to the output\n",
        "        # to get probabilities for the BCEWithLogitsLoss function.\n",
        "        if activation not in ['sigmoid', 'tanh']:\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "        # Calculate the loss.\n",
        "        loss = criterion(outputs, y_train)\n",
        "        # Perform backpropagation to calculate gradients.\n",
        "        loss.backward()\n",
        "        # Update model parameters.\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model's performance on the test data.\n",
        "    model.eval() # Set the model to evaluation mode.\n",
        "    with torch.no_grad(): # Disable gradient calculation for efficiency.\n",
        "        # Get predictions on the test set.\n",
        "        test_outputs = model(X_test)\n",
        "        # Apply sigmoid to outputs if needed for a probability-like range.\n",
        "        if activation not in ['sigmoid', 'tanh']:\n",
        "            test_outputs = torch.sigmoid(test_outputs)\n",
        "        # Convert probabilities to binary predictions (0 or 1).\n",
        "        preds = (test_outputs > 0.5).float()\n",
        "        # Calculate the accuracy by comparing predictions to true labels.\n",
        "        acc = (preds == y_test).float().mean().item()\n",
        "\n",
        "    # Perform an analysis of the eigenvalues of the first layer's weight matrix.\n",
        "    # This analysis helps to understand the \"shape\" of the data transformation.\n",
        "    W = model.fc1.weight.data\n",
        "    # Calculate the Gram matrix (W.T @ W), which is a positive semi-definite matrix.\n",
        "    # Its eigenvalues are related to the singular values of W, indicating the\n",
        "    # variance captured along different dimensions.\n",
        "    gram = W.T @ W\n",
        "    # Compute the real parts of the eigenvalues of the Gram matrix.\n",
        "    eigenvalues = torch.linalg.eigvals(gram).real\n",
        "    print(f\"Eigenvalues for {activation}: {eigenvalues.tolist()}\")\n",
        "\n",
        "    # Return the calculated accuracy.\n",
        "    return acc\n",
        "\n",
        "# Train and get accuracies for each activation function.\n",
        "linear_acc = train_and_evaluate_ext('linear')\n",
        "relu_acc = train_and_evaluate_ext('relu')\n",
        "leaky_relu_acc = train_and_evaluate_ext('leaky_relu')\n",
        "silu_acc = train_and_evaluate_ext('silu')\n",
        "sigmoid_acc = train_and_evaluate_ext('sigmoid')\n",
        "tanh_acc = train_and_evaluate_ext('tanh')\n",
        "\n",
        "# Print the final validation accuracies.\n",
        "print(f\"Linear Validation Accuracy: {linear_acc}\")\n",
        "print(f\"ReLU Validation Accuracy: {relu_acc}\")\n",
        "print(f\"SiLU Validation Accuracy: {silu_acc}\")\n",
        "print(f\"Sigmoid Validation Accuracy: {sigmoid_acc}\")\n",
        "print(f\"Tanh Validation Accuracy: {tanh_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nrg1c7pn1C3",
        "outputId": "b48f6ca5-cd49-4a9d-b3b1-6e5fe56b4fb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues for linear: [0.08079659938812256, 2.4849889278411865]\n",
            "Eigenvalues for relu: [0.7510960698127747, 1.9164135456085205]\n",
            "Eigenvalues for leaky_relu: [4.6180195808410645, 0.44533294439315796]\n",
            "Eigenvalues for silu: [0.19552457332611084, 1.3364933729171753]\n",
            "Eigenvalues for sigmoid: [0.6227757930755615, 1.6903197765350342]\n",
            "Eigenvalues for tanh: [0.2578141689300537, 1.9096938371658325]\n",
            "Linear Validation Accuracy: 0.5\n",
            "ReLU Validation Accuracy: 1.0\n",
            "SiLU Validation Accuracy: 0.5\n",
            "Sigmoid Validation Accuracy: 0.5\n",
            "Tanh Validation Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Probabilistic Implementations**"
      ],
      "metadata": {
        "id": "qKH18u_aoSy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Expectations and Variance Computation\n",
        "\n",
        "**Expectation** is just another word for the average. It's the number you would \"expect\" to get if you took the average of all the data points.\n",
        "\n",
        "Imagine you have a class of 10 students and you want to know the average height. You measure each student's height, add them all up, and divide by 10. That final number is the average height, or the expectation of the height for a student in that class. If you were to pick a student at random, you'd \"expect\" their height to be close to this average.\n",
        "\n",
        "**Variance** tells you how spread out the data is from the average.  A small variance means the data points are all close together, while a large variance means they are very spread out.\n",
        "\n",
        "*   **Low Variance:** If all 10 students are almost exactly the same height (e.g., they are all between 5'5\" and 5'7\"), their height has a low variance.\n",
        "*   **High Variance:** If some students are very short (e.g., 4'0\") and others are very tall (e.g., 6'8\"), their height has a high variance.\n",
        "\n",
        "Variance is useful because it measures the variability or risk. In a dataset, high variance means the values are unpredictable and can vary a lot, which is crucial information for a machine learning model."
      ],
      "metadata": {
        "id": "b28QUjwVoqcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expectation E(X) for features (discrete approx)\n",
        "# Calculate the mean (expectation) of the 'Humidity' feature.\n",
        "exp_hum = torch.mean(X[:, 0]).item()\n",
        "# Calculate the mean (expectation) of the 'Temperature' feature.\n",
        "exp_temp = torch.mean(X[:, 1]).item()\n",
        "print(f\"E(Humidity): {exp_hum}, E(Temperature): {exp_temp}\")\n",
        "\n",
        "# Variance Var(X)\n",
        "# Calculate the variance of the 'Humidity' feature.\n",
        "var_hum = torch.var(X[:, 0]).item()\n",
        "# Calculate the variance of the 'Temperature' feature.\n",
        "var_temp = torch.var(X[:, 1]).item()\n",
        "print(f\"Var(Humidity): {var_hum}, Var(Temperature): {var_temp}\")\n",
        "\n",
        "# Covariance Cov(Humidity, Temperature)\n",
        "# Calculate the covariance matrix for the features.\n",
        "# The .T transposes the tensor so that each column represents a feature, which is the required format for torch.cov.\n",
        "cov_matrix = torch.cov(X.T)\n",
        "# Print the covariance matrix as a list.\n",
        "print(\"Covariance matrix:\\n\", cov_matrix.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8BuuPNJmV38",
        "outputId": "fb95cb84-4275-4bd9-8e40-bc4b0b75dfc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E(Humidity): 54.099998474121094, E(Temperature): 20.700000762939453\n",
            "Var(Humidity): 378.54443359375, Var(Temperature): 17.344444274902344\n",
            "Covariance matrix:\n",
            " [[378.54449462890625, 13.922221183776855], [13.922221183776855, 17.344446182250977]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Conditional Distributions and Independence Check\n",
        "\n",
        "**Conditional Distributions**:\n",
        "A conditional distribution looks at a specific group within your data to find the average. It helps you answer questions like, \"What's the probability of something happening, GIVEN that something else is true?\"\n",
        "\n",
        "Imagine you want to know the probability of rain. A simple average might be 50%. But if you look at a specific condition—\"What's the probability of rain GIVEN that the humidity is high?\"—you're likely to get a much higher number, say 90%. This gives you a more accurate and useful prediction.\n",
        "\n",
        "**Independence Check**:\n",
        "Independence is a concept in probability where knowing the outcome of one event tells you nothing about the outcome of another. You can check for approximate independence by looking at the covariance between two variables. If the covariance is close to zero, it suggests the two events are independent.\n",
        "\n",
        "*   **Independent Events:** The amount of rain you get this month is likely independent of the stock market's performance. Knowing that it rained a lot doesn't help you predict if the stock market went up or down. Their covariance would be close to zero.\n",
        "*   **Dependent Events:** The amount of rain you get is dependent on the humidity. When humidity is high, the probability of rain is also high. Their covariance would be a large positive number, indicating they are linked."
      ],
      "metadata": {
        "id": "eq2KUuxEo-fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional expectation E(Rain|Humidity > mean)\n",
        "# Create a boolean mask to select data points where humidity is greater than the calculated mean.\n",
        "high_hum_mask = X[:, 0] > exp_hum\n",
        "# Calculate the conditional expectation (mean) of 'Rain' for only those data points\n",
        "# that meet the high humidity condition defined by the mask.\n",
        "cond_exp_rain_high_hum = torch.mean(y[high_hum_mask]).item()\n",
        "print(f\"E(Rain|Humidity > mean): {cond_exp_rain_high_hum}\")\n",
        "\n",
        "# Check approximate independence (cov ~0?)\n",
        "# Calculate the covariance between 'Humidity' (X[:, 0]) and 'Rain' (y).\n",
        "# torch.cat combines the two tensors, and .T ensures they are treated as rows for the covariance calculation.\n",
        "cov_hum_rain = torch.cov(torch.cat((X[:, 0].unsqueeze(0), y.T), dim=0))[0, 1].item()\n",
        "# A covariance close to 0 suggests the variables are approximately independent.\n",
        "print(f\"Cov(Humidity, Rain) ~0 for independence: {cov_hum_rain}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPkXEkoo1o4",
        "outputId": "88325feb-9567-41ed-fafc-cd07afd98c0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E(Rain|Humidity > mean): 0.6666666865348816\n",
            "Cov(Humidity, Rain) ~0 for independence: 5.611111164093018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Bayes Rule Approximation\n",
        "\n",
        "Bayes' Rule is a way to update your beliefs about an event when you get new evidence. It helps you figure out the probability of something happening based on what you already knew and what you've just observed.\n",
        "\n",
        "Think of it like being a detective. Your initial guess is the prior. The new clue you find is the evidence. Bayes' Rule helps you use that clue to make a much better, more informed guess (the posterior)."
      ],
      "metadata": {
        "id": "BFJzxGFNpFJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Priors\n",
        "# Calculate the prior probability of rain (P(Rain)), which is the overall proportion of rainy days in the dataset.\n",
        "p_rain = torch.mean(y).item()\n",
        "# Calculate the prior probability of high humidity (P(Humidity high)), which is the proportion of days with humidity above the average.\n",
        "p_hum_high = (X[:, 0] > exp_hum).float().mean().item()\n",
        "\n",
        "# Likelihood P(Rain|Humidity high)\n",
        "# Calculate the likelihood: the probability of rain given that humidity is high.\n",
        "# This is the same as the conditional expectation calculated earlier.\n",
        "p_rain_given_high_hum = torch.mean(y[high_hum_mask]).item()\n",
        "\n",
        "# Posterior\n",
        "# Calculate the posterior probability P(High Humidity|Rain) using Bayes' Theorem.\n",
        "# The formula is: P(A|B) = [P(B|A) * P(A)] / P(B)\n",
        "# Here, A = 'High Humidity' and B = 'Rain'.\n",
        "p_high_hum_given_rain = (p_rain_given_high_hum * p_hum_high) / p_rain\n",
        "print(f\"P(High Humidity|Rain) via Bayes: {p_high_hum_given_rain}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LzZOeZ7pEf0",
        "outputId": "4fe8279a-32ad-4de3-a5a0-e8c40f4aa465"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(High Humidity|Rain) via Bayes: 0.8000000556310027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. Sampling from Distributions (Bernoulli, Poisson, Gaussian)\n",
        "\n",
        "**Bernoulli Distribution**: This distribution models a simple event with two outcomes, like a coin flip. You specify the probability of one outcome (e.g., heads), and then you can sample to get a series of heads or tails results. It's used for binary classification outcomes.\n",
        "\n",
        "**Poisson Distribution**: This distribution is used for counting how many times an event occurs in a fixed period, like the number of phone calls to a call center in an hour. You provide the average rate of occurrence, and the distribution gives you a likely count. It's used for modeling rare events.\n",
        "\n",
        "**Gaussian (Normal) Distribution**: Also known as the \"bell curve,\" this is a continuous distribution where data points cluster around an average value. You define the mean and spread (variance), and you can sample to generate new, realistic data points that follow this pattern. It's widely used for modeling natural phenomena like height or temperature."
      ],
      "metadata": {
        "id": "kU1eJF8JpdJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bernoulli for simulated labels\n",
        "# Define the probability for the Bernoulli distribution. This can be seen as the\n",
        "# overall probability of 'Rain' from the original dataset.\n",
        "bern_p = 0.5\n",
        "# Create a Bernoulli distribution object with the defined probability.\n",
        "# .sample((10, 1)) draws 10 samples (e.g., simulating 10 days) from this distribution,\n",
        "# with each sample being a binary outcome (1 for success/rain, 0 for failure/no rain).\n",
        "bern_samples = torch.distributions.Bernoulli(probs=bern_p).sample((10, 1))\n",
        "print(\"Bernoulli samples (sim rain):\\n\", bern_samples.tolist())\n",
        "\n",
        "# Poisson for simulated event counts (e.g., rain days)\n",
        "# Define the rate parameter (lambda) for the Poisson distribution, which represents\n",
        "# the average number of events (e.g., rainy days) in a fixed interval.\n",
        "pois_lambda = 2.0\n",
        "# Create a Poisson distribution and draw 10 samples. Each sample is a non-negative\n",
        "# integer representing the number of events.\n",
        "pois_samples = torch.distributions.Poisson(rate=pois_lambda).sample((10,))\n",
        "print(\"Poisson samples:\\n\", pois_samples.tolist())\n",
        "\n",
        "# Gaussian for noisy features\n",
        "# Define the mean vector for the Gaussian (Normal) distribution. This vector\n",
        "# contains the mean of each feature ('Humidity' and 'Temperature').\n",
        "gauss_mean = torch.tensor([exp_hum, exp_temp])\n",
        "# Define the covariance matrix for the distribution, which was calculated earlier.\n",
        "# This matrix captures the variance of each feature and the covariance between them.\n",
        "gauss_cov = cov_matrix\n",
        "# Create a MultivariateNormal distribution object using the mean vector and covariance matrix.\n",
        "gauss_dist = torch.distributions.MultivariateNormal(gauss_mean, gauss_cov)\n",
        "# Draw 5 samples from this multivariate Gaussian distribution. Each sample is a\n",
        "# vector with two components, simulating a pair of 'Humidity' and 'Temperature' values.\n",
        "gauss_samples = gauss_dist.sample((5,))\n",
        "print(\"Gaussian samples:\\n\", gauss_samples.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6oaN_d_pQ1n",
        "outputId": "6307bf9e-ca6f-4fb8-9c7c-6bb641cdf154"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli samples (sim rain):\n",
            " [[0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0]]\n",
            "Poisson samples:\n",
            " [1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 0.0, 3.0, 3.0, 1.0]\n",
            "Gaussian samples:\n",
            " [[50.76548385620117, 25.178524017333984], [39.170654296875, 18.41327476501465], [50.60792541503906, 22.319992065429688], [72.70157623291016, 27.481904983520508], [83.90538787841797, 25.1619930267334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e. Jensen's Inequality Demo\n",
        "\n",
        "Jensen's Inequality states that for a convex function like f(x)=x^2, the average of the function's output is always greater than or equal to the function of the average input.\n",
        "\n",
        "Think of a group of people's incomes. If you calculate the average income and then square it, you get one number. But if you square each person's income first and then find the average of those squared values, that number will always be larger. Jensen's Inequality shows that for a convex function like squaring, the average of the outputs is always greater than or equal to the output of the average."
      ],
      "metadata": {
        "id": "8q289XzaqtCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convex function f(x) = x^2, apply to humidity\n",
        "# Select the humidity values from the feature tensor X.\n",
        "hum_vals = X[:, 0]\n",
        "# Calculate the expectation of the function: E(f(X)).\n",
        "# This involves squaring each humidity value first, and then taking the mean of the results.\n",
        "exp_f = torch.mean(hum_vals ** 2).item()\n",
        "# Calculate the function of the expectation: f(E(X)).\n",
        "# This involves taking the mean of the humidity values first, and then squaring the result.\n",
        "f_exp = (torch.mean(hum_vals) ** 2).item()\n",
        "# Print the results to demonstrate Jensen's Inequality, which states that E[f(X)] >= f[E(X)] for a convex function f.\n",
        "print(f\"Jensen: E(f(X))={exp_f} >= f(E(X))={f_exp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SPYQwXDqpyn",
        "outputId": "e2c33eb9-5a2e-46f1-df6e-7cef20fb3c5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jensen: E(f(X))=3267.5 >= f(E(X))=2926.809814453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linear Algebra Implementations**"
      ],
      "metadata": {
        "id": "0BObpHMRqxCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Trace, Determinant, and Inverse\n",
        "\n",
        "**Trace**:\n",
        "The trace of a matrix is the sum of its diagonal elements. In the context of a covariance matrix, the trace represents the total variance of all the features combined, giving a single number that summarizes the overall spread of your data.\n",
        "\n",
        "**Determinant**:\n",
        "The determinant of a matrix tells you if the matrix is invertible and how much the space is \"scaled\" by the transformation the matrix represents. For a covariance matrix, a positive determinant confirms that the matrix is valid and that the features are not perfectly redundant. A determinant of zero would mean at least one feature is a perfect linear combination of the others.\n",
        "\n",
        "**Inverse**:\n",
        "The inverse of a matrix, if it exists, is a second matrix that \"undoes\" the transformation of the first. In statistics, the inverse of the covariance matrix (called the precision matrix) is used to calculate things like the probability density of a multivariate Gaussian distribution."
      ],
      "metadata": {
        "id": "rIdumNnYq3a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cov matrix from prob section\n",
        "# Calculate and print the trace of the covariance matrix.\n",
        "# The trace is the sum of the diagonal elements, which represents the total variance of the features.\n",
        "print(f\"Trace of cov: {torch.trace(cov_matrix).item()}\")\n",
        "\n",
        "# Determinant (for PSD check, |cov| >0)\n",
        "# Calculate the determinant of the covariance matrix.\n",
        "# A positive determinant is a condition for the matrix to be positive semi-definite (PSD),\n",
        "# which is required for it to be a valid covariance matrix.\n",
        "det_cov = torch.det(cov_matrix).item()\n",
        "print(f\"Det of cov: {det_cov}\")\n",
        "\n",
        "# Inverse (if invertible)\n",
        "# Check if the determinant is non-zero. A non-zero determinant means the matrix is invertible.\n",
        "if det_cov != 0:\n",
        "    # Calculate and print the inverse of the covariance matrix.\n",
        "    # The inverse is used in various statistical and machine learning applications,\n",
        "    # such as in the formula for the multivariate Gaussian probability density function.\n",
        "    inv_cov = torch.inverse(cov_matrix)\n",
        "    print(\"Inverse cov:\\n\", inv_cov.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYEJy8JAqwA_",
        "outputId": "84b59919-51ac-4ad6-c4da-9ce38d439801"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace of cov: 395.8889465332031\n",
            "Det of cov: 6371.81640625\n",
            "Inverse cov:\n",
            " [[0.0027220568154007196, -0.0021849689073860645], [-0.002184969140216708, 0.05940919741988182]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Norms and Rank\n",
        "\n",
        "**Norms** measure the \"length\" or \"magnitude\" of a vector. The L2 norm is the standard Euclidean distance from the origin, calculated as the square root of the sum of the squared components. The L1 norm is the sum of the absolute values of the components, often called the Manhattan distance.\n",
        "\n",
        "The **rank** of a matrix tells you the number of its linearly independent columns or rows. A matrix has full rank if its rank is equal to the smaller of its number of rows or columns. It's a key concept in linear algebra, indicating if the data has any redundant features."
      ],
      "metadata": {
        "id": "74kOLg7Kq7TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 norm of first feature vector\n",
        "# Calculate the L2 norm (Euclidean distance from the origin) of the first data sample.\n",
        "# This represents the length or magnitude of the vector containing the first day's humidity and temperature.\n",
        "l2_norm = torch.norm(X[0]).item()\n",
        "print(f\"L2 norm of first sample: {l2_norm}\")\n",
        "\n",
        "# L1 norm\n",
        "# Calculate the L1 norm (Manhattan distance) of the same data sample.\n",
        "# The 'p=1' argument specifies the L1 norm. It's the sum of the absolute values of the vector's components.\n",
        "l1_norm = torch.norm(X[0], p=1).item()\n",
        "print(f\"L1 norm: {l1_norm}\")\n",
        "\n",
        "# Matrix rank (full rank=2 for 2 features)\n",
        "# Calculate the rank of the data matrix. The rank indicates the number of linearly independent\n",
        "# columns (or rows). A full rank means no feature is a perfect linear combination of the others.\n",
        "# The Gram matrix (X.T @ X) is often used for numerical stability when calculating rank.\n",
        "rank_X = torch.linalg.matrix_rank(X.T @ X).item()\n",
        "print(f\"Rank of data matrix: {rank_X}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW5IwCE-q6On",
        "outputId": "e0d29beb-d5bd-4d45-841c-445e85eef89e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 norm of first sample: 82.46211242675781\n",
            "L1 norm: 100.0\n",
            "Rank of data matrix: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Eigenvalues/Eigenvectors and Quadratic Form\n",
        "\n",
        "**Eigenvalues & Eigenvectors**:\n",
        "Imagine you have a rubber sheet you're stretching. Eigenvectors are the specific lines you've drawn on the sheet that don't change their direction when you stretch it, even though their length might change. The eigenvalue is the number that tells you how much that specific line got stretched. In data, they show the directions of greatest variance.\n",
        "\n",
        "**Quadratic Form**:\n",
        "A quadratic form is a calculation that tells you the shape of a surface. Think of it as a blueprint for an ellipse or an ellipsoid. For example, the equation **ax^2 + by^2 = 1** is a quadratic form that draws an ellipse. In machine learning, this calculation is used to define the shape of a bell curve in multiple dimensions, which is crucial for probability distributions."
      ],
      "metadata": {
        "id": "whS-N2wFrBxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eigen decomp of cov (symmetric, relates to PSD)\n",
        "# Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
        "# The `eigh` function is used for symmetric matrices, which covariance matrices are.\n",
        "# Eigenvalues must be non-negative for a valid covariance matrix (Positive Semi-Definite).\n",
        "eigvals, eigvecs = torch.linalg.eigh(cov_matrix)\n",
        "print(\"Eigenvalues:\\n\", eigvals.tolist())\n",
        "print(\"Eigenvectors:\\n\", eigvecs.tolist())\n",
        "\n",
        "# Quadratic form x^T A x (A=cov, x=first sample)\n",
        "# Calculate the quadratic form using the first data sample vector (X[0]) and the covariance matrix.\n",
        "# This operation (x^T * A * x) results in a scalar and is used in many machine learning\n",
        "# contexts, such as the exponent of a multivariate Gaussian probability density function.\n",
        "quad_form = (X[0].T @ cov_matrix @ X[0]).item()\n",
        "print(f\"Quadratic form: {quad_form}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrVB12-lrGBq",
        "outputId": "f7e73300-e51d-4db5-cd35-fae4a34a683f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            " [16.808618545532227, 379.080322265625]\n",
            "Eigenvectors:\n",
            " [[0.038458775728940964, -0.9992601275444031], [-0.9992601275444031, -0.038458775728940964]]\n",
            "Quadratic form: 2474173.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3566382883.py:13: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4421.)\n",
            "  quad_form = (X[0].T @ cov_matrix @ X[0]).item()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. Matrix Transpose and Hadamard Product\n",
        "\n",
        "The **transpose of a matrix** simply flips it over its diagonal, turning its rows into columns and its columns into rows. It's a fundamental operation in linear algebra, often used to align dimensions for matrix multiplication.\n",
        "\n",
        "The **Hadamard product** (or element-wise product) is a multiplication where two matrices of the same size are multiplied element by element. It's a straightforward operation, unlike standard matrix multiplication, as it doesn't involve rows and columns interactions."
      ],
      "metadata": {
        "id": "Zb63sJ0nrxkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose of X\n",
        "# Transpose the feature tensor X. The rows become columns and the columns become rows.\n",
        "# This operation is often needed for matrix multiplication (e.g., X.T @ X) or for\n",
        "# functions like torch.cov that expect features in rows.\n",
        "X_transpose = X.T\n",
        "print(\"X transpose shape:\", X_transpose.shape)\n",
        "\n",
        "# Hadamard product (element-wise) of first two rows\n",
        "# Perform an element-wise multiplication (Hadamard product) of the first two rows (samples) of X.\n",
        "# This means the first element of row 0 is multiplied by the first element of row 1, and so on.\n",
        "hadamard_prod = X[0] * X[1]\n",
        "print(\"Hadamard product of rows 0 and 1:\\n\", hadamard_prod.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrOgL_lNr0E_",
        "outputId": "96758454-87af-4213-9a3d-4da422fcacad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X transpose shape: torch.Size([2, 10])\n",
            "Hadamard product of rows 0 and 1:\n",
            " [1600.0, 300.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e. Orthogonal Matrix Construction"
      ],
      "metadata": {
        "id": "RGxcUVD0sM39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a random matrix (e.g., from data covariance or weights)\n",
        "# Create a new 2x2 matrix by adding random noise to the covariance matrix.\n",
        "# This serves as a general matrix to demonstrate QR decomposition.\n",
        "random_mat = cov_matrix + torch.randn(2, 2)\n",
        "\n",
        "# QR decomposition to get orthogonal Q\n",
        "# Perform QR decomposition on the random matrix.\n",
        "# This decomposes the matrix into an orthogonal matrix Q and an upper triangular matrix R.\n",
        "Q, R = torch.linalg.qr(random_mat)\n",
        "print(\"Orthogonal Matrix Q:\\n\", Q.tolist())\n",
        "\n",
        "# Verify orthogonality: Q^T Q should be identity\n",
        "# Multiply the transpose of Q by Q to verify its orthogonality.\n",
        "# For an orthogonal matrix, this product should result in the identity matrix.\n",
        "Q_t_Q = Q.T @ Q\n",
        "print(\"Q^T Q (should be identity):\\n\", Q_t_Q.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmubcb0MsN0O",
        "outputId": "66e67df5-7a03-4318-8cb0-d0bf5facee85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal Matrix Q:\n",
            " [[-0.9993083477020264, -0.03718753904104233], [-0.03718753904104233, 0.9993082880973816]]\n",
            "Q^T Q (should be identity):\n",
            " [[1.0000001192092896, 3.725290298461914e-09], [3.725290298461914e-09, 0.9999999403953552]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task: Build, Train, and Evaluate a Three-Layer Neural Network**\n",
        "\n",
        "Your task is to complete the code below to build and train a neural network that predicts rain. You will define a training function, use both the ReLU and SiLU activation functions, and plot the final accuracy to compare their performance."
      ],
      "metadata": {
        "id": "5kCRUTyXrZAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset Preparation (Provided for context)\n",
        "data = \"\"\"Day No.,Humidity  (%),Temperature (degree Celsius),Rain\n",
        "1,80,20,Yes\n",
        "2,20,15,No\n",
        "3,70,25,Yes\n",
        "4,30,24,Yes\n",
        "5,55,28,No\n",
        "6,68,19,Yes\n",
        "7,45,21,No\n",
        "8,73,17,Yes\n",
        "9,44,16,No\n",
        "10,56,22,No\"\"\"\n",
        "\n",
        "df = pd.read_csv(io.StringIO(data))\n",
        "df['Rain'] = df['Rain'].map({'Yes': 1, 'No': 0})\n",
        "X = torch.tensor(df[['Humidity  (%)', 'Temperature (degree Celsius)']].values, dtype=torch.float32)\n",
        "y = torch.tensor(df['Rain'].values, dtype=torch.float32).unsqueeze(1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "SEplFYqjrX7G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Section 1: Define the Three-Layer Neural Network -----\n",
        "class ThreeLayerNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, activation):\n",
        "        super(ThreeLayerNet, self).__init__()\n",
        "        # TODO: Define the three linear layers\n",
        "        self.fc1 =\n",
        "        self.fc2 =\n",
        "        self.fc3 =\n",
        "        self.activation = activation.lower()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Pass the input through the first layer\n",
        "        x =\n",
        "\n",
        "        # TODO: Implement the activation function logic here.\n",
        "        if self.activation == 'relu':\n",
        "            # TODO: Apply ReLU\n",
        "            x =\n",
        "        elif self.activation == 'silu':\n",
        "            # TODO: Apply SiLU\n",
        "            x =\n",
        "\n",
        "        # TODO: Pass through the second and third layers\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ajDfkpKcrl3r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Section 2: Training and Evaluation Function -----\n",
        "def train_and_evaluate(activation):\n",
        "    \"\"\"Trains and evaluates a model with the specified activation function.\"\"\"\n",
        "    model = ThreeLayerNet(input_size=2, hidden_size1=8, hidden_size2=4, output_size=1, activation=activation)\n",
        "\n",
        "    # TODO: Define the optimizer (e.g., Adam) for the model parameters with a learning rate of 0.01.\n",
        "\n",
        "\n",
        "    # TODO: Define the loss function (BCEWithLogitsLoss is suitable for binary classification).\n",
        "\n",
        "\n",
        "    # Lists to store metrics for plotting\n",
        "    train_losses = []\n",
        "    eval_accuracies = []\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(200):\n",
        "        model.train()\n",
        "\n",
        "        # TODO: Zero the gradients.\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_train)\n",
        "\n",
        "        # TODO: Calculate the loss.\n",
        "\n",
        "\n",
        "        # TODO: Backward pass and optimization\n",
        "\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Evaluation Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test)\n",
        "            test_preds = (torch.sigmoid(test_outputs) > 0.5).float()\n",
        "            accuracy = (test_preds == y_test).float().mean().item()\n",
        "            eval_accuracies.append(accuracy)\n",
        "\n",
        "    return eval_accuracies, train_losses"
      ],
      "metadata": {
        "id": "tS9wSSTtrqtJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Section 3: Run the Experiment and Plot Results -----\n",
        "\n",
        "# Run the training and evaluation for both activation functions\n",
        "relu_accuracies, relu_losses = train_and_evaluate('relu')\n",
        "silu_accuracies, silu_losses = train_and_evaluate('silu')"
      ],
      "metadata": {
        "id": "9R2eqI_GruMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# TODO: Plot the ReLU training loss.\n",
        "\n",
        "\n",
        "# TODO: Plot the SiLU training loss.\n",
        "\n",
        "\n",
        "plt.title('Training Loss per Epoch for Different Activation Functions')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting the validation accuracy curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# TODO: Plot the ReLU validation accuracy.\n",
        "\n",
        "\n",
        "# TODO: Plot the SiLU validation accuracy.\n",
        "\n",
        "\n",
        "plt.title('Validation Accuracy per Epoch for Different Activation Functions')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C3khH12qrxc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}